import tensorflow as tf
import numpy as np
import cv2
import os
import json
import tkinter as tk
from tkinter import filedialog, messagebox
from PIL import Image, ImageGrab
import argparse

# --- 1. é…ç½®èˆ‡åƒæ•¸è¨­å®š ---
MODEL_SAVE_PATH = "trained_model_tf"
CLASS_INDICES_FILE = "class_indices_cifar10.json"
IMAGE_SIZE = (224, 224)
IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)
IMAGENET_STD = np.array([0.229, 0.224, 0.225], dtype=np.float32)

# åˆ¤å®šé–€æª» (å»¶çºŒæ‚¨çš„ Margin é‚è¼¯)
MARGIN_THRESHOLD = 0.05
OPEN_BTN_RECT = (20, 450, 120, 40)

# --- 2. TFLite æ¨è«–è¼”åŠ©å‡½å¼ ---
class TFLiteInference:
    def __init__(self, model_path):
        print(f"ğŸ“¦ æ­£åœ¨è¼‰å…¥ TFLite æ¨¡å‹: {model_path}")
        # ä¸»è¦çš„æ”¹è®Šåœ¨æ–¼ä½¿ç”¨ tf.lite.Interpreter å–ä»£ tf.keras.models.load_model
        self.interpreter = tf.lite.Interpreter(model_path=model_path)
        # æ ¹æ“šæ¨¡å‹çµæ§‹ï¼Œæ­£å¼åœ¨è¨˜æ†¶é«”ï¼ˆRAMï¼‰ä¸­é–‹é—¢ç©ºé–“ï¼Œç”¨ä¾†å­˜æ”¾è¼¸å…¥è³‡æ–™ã€ä¸­é–“å±¤çš„é‹ç®—çµæœï¼ˆActivationsï¼‰ä»¥åŠæœ€çµ‚è¼¸å‡ºã€‚
        # é€™æ˜¯ä¸€é“å¿…é ˆåŸ·è¡Œçš„æŒ‡ä»¤ã€‚å¦‚æœä½ æ²’æœ‰å‘¼å«å®ƒï¼Œå¾Œé¢å˜—è©¦ set_tensorï¼ˆæŠŠåœ–ç‰‡ä¸Ÿé€²å»ï¼‰æˆ– invokeï¼ˆåŸ·è¡Œæ¨è«–ï¼‰æ™‚ï¼Œç¨‹å¼æœƒå ±éŒ¯ã€‚
        self.interpreter.allocate_tensors()

        # å–å¾—è¼¸å…¥èˆ‡è¼¸å‡ºå¼µé‡çš„ç´°ç¯€. input_details å’Œ output_details æ˜¯ list of dicts ï¼Œè£¡é¢åŒ…å«æ¯å€‹è¼¸å…¥/è¼¸å‡ºçš„è³‡è¨Š.
        # å³ä½¿ä½ çš„æ¨¡å‹åªæœ‰ä¸€å€‹è¼¸å…¥å’Œä¸€å€‹è¼¸å‡ºï¼Œå®ƒå€‘ä»ç„¶ä»¥ list[0] çš„å½¢å¼å­˜åœ¨ã€‚
        # ex. self.input_details[0] (å…¥å£è¦æ ¼)
        # {
        #    'name': 'serving_default_input_1:0',
        #    'index': 0,                                      # è©²å¼µé‡åœ¨ interpreter è£¡çš„ç´¢å¼•, ç”¨æ–¼ set_tensor(index, data)ã€‚
        #    'shape': array([  1, 224, 224,   3], dtype=int32), # è©²å¼µé‡çš„ç¶­åº¦, batch, height, width, channels.
        #    'dtype': <class 'numpy.float32'>,                # æ•¸æ“šé¡å‹ï¼ˆå¦‚ numpy.float32, numpy.uint8, numpy.int8ï¼‰
        #    'quantization': (0.0, 0)                         # (scale, zero_point). éé‡åŒ–æ¨¡å‹ç‚º (0.0, 0). 
        # }
        # ex. self.output_details[0] (å‡ºå£è¦æ ¼)
        # {
        #    'name': 'StatefulPartitionedCall:0',
        #    'index': 175,
        #    'shape': array([ 1, 10], dtype=int32),           # <-- å½¢ç‹€åœ¨é€™è£¡. batch, num_classes.
        #    'dtype': <class 'numpy.float32'>,
        #    'quantization': (0.0, 0)
        # }
        self.input_details = self.interpreter.get_input_details()
        self.output_details = self.interpreter.get_output_details()
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºé‡åŒ–æ¨¡å‹ (INT8 æ¨¡å‹é€šå¸¸è¼¸å…¥ç‚º uint8)
        self.input_dtype = self.input_details[0]['dtype']
        print(f"ğŸ’¡ æ¨¡å‹è¼¸å…¥é¡å‹: {self.input_dtype}")

    def predict(self, pil_img):
        # A. é è™•ç† (èˆ‡è¨“ç·´ä¸€è‡´)
        img = pil_img.convert('RGB').resize(IMAGE_SIZE)
        # torch çš„ transforms æ¥å— pillow image. 
        # åœ¨tensorflow è£¡ pillow image éœ€è¦å…ˆè½‰æˆ numpy array å¾Œåšæ¨™æº–åŒ–.
        img_array = np.array(img).astype(np.float32) / 255.0
        img_array = (img_array - IMAGENET_MEAN) / IMAGENET_STD
        input_data = np.expand_dims(img_array, axis=0)  # å¢åŠ  batch ç¶­åº¦, è®Šæˆ (1, height, width, channels)

        # B. å¦‚æœæ¨¡å‹æ˜¯ INT8 é‡åŒ–ï¼Œå¯èƒ½éœ€è¦æ ¡æº–è¼¸å…¥æ•¸æ“šé¡å‹
        # éœ€è¦å°‡å‰›æ‰ç®—å¥½çš„æµ®é»æ•¸ input_data é€éå…¬å¼ï¼šQ = R / S + Z è½‰æˆæ•´æ•¸, Q æ˜¯é‡åŒ–å€¼ï¼ŒR æ˜¯åŸå§‹å€¼. S æ˜¯ scaleï¼ŒZ æ˜¯ zero_point.
        if self.input_dtype == np.uint8:
            # TFLite çš„ uint8 é‡åŒ–é€šå¸¸æœ‰ scale å’Œ zero_point
            input_scale, input_zero_point = self.input_details[0]['quantization']
            input_data = (input_data / input_scale + input_zero_point).astype(np.uint8)
        elif self.input_dtype == np.int8:
            input_scale, input_zero_point = self.input_details[0]['quantization']
            input_data = (input_data / input_scale + input_zero_point).astype(np.int8)

        # C. åŸ·è¡Œæ¨è«–
        self.interpreter.set_tensor(self.input_details[0]['index'], input_data)
        self.interpreter.invoke()

        # D. å–å¾—è¼¸å‡ºä¸¦è½‰å›æ©Ÿç‡ (Softmax)
        output_data = self.interpreter.get_tensor(self.output_details[0]['index'])[0]
        
        # å¦‚æœè¼¸å‡ºæ˜¯é‡åŒ–æ•´æ•¸ï¼Œä¹Ÿéœ€è¦è½‰å›æµ®é»æ•¸
        if self.output_details[0]['dtype'] in [np.uint8, np.int8]:
            output_scale, output_zero_point = self.output_details[0]['quantization']
            output_data = (output_data.astype(np.float32) - output_zero_point) * output_scale
        
        return output_data

def select_model_file():
    root = tk.Tk(); root.withdraw()
    path = filedialog.askopenfilename(
        title="é¸æ“‡ TFLite æ¨¡å‹æª”",
        initialdir=MODEL_SAVE_PATH,
        filetypes=[("TFLite files", "*.tflite")]
    )
    root.destroy()
    return path

def run_logic(tflite_engine, pil_img, class_names):
    predictions = tflite_engine.predict(pil_img)
    
    # Margin åˆ¤æ–·é‚è¼¯
    top_indices = np.argsort(predictions)[-2:][::-1]
    top1_idx, top2_idx = top_indices[0], top_indices[1]
    top1_prob, top2_prob = predictions[top1_idx], predictions[top2_idx]
    
    margin = top1_prob - top2_prob
    if margin < MARGIN_THRESHOLD:
        return "Unknown", top1_prob * 100
    else:
        return class_names[str(top1_idx)], top1_prob * 100

def main():
    # 1. é¸æ“‡ TFLite æª”æ¡ˆ
    tflite_path = select_model_file()
    if not tflite_path: return

    # 2. è¼‰å…¥é¡åˆ¥å­—å…¸
    with open(os.path.join(MODEL_SAVE_PATH, CLASS_INDICES_FILE), 'r') as f:
        class_names = {str(v): k for k, v in json.load(f).items()}

    # 3. åˆå§‹åŒ– TFLite å¼•æ“
    engine = TFLiteInference(tflite_path)
    win_name = f'TFLite Test: {os.path.basename(tflite_path)}'
    cv2.namedWindow(win_name)

    # 4. æ»‘é¼ äº‹ä»¶èˆ‡è®Šæ•¸åˆå§‹åŒ–
    params = {'clicked_open': False}
    def on_mouse(event, x, y, flags, param):
        if event == cv2.EVENT_LBUTTONDOWN:
            bx, by, bw, bh = OPEN_BTN_RECT
            if bx <= x <= bx + bw and by <= y <= by + bh: param['clicked_open'] = True
    cv2.setMouseCallback(win_name, on_mouse, params)

    last_clipboard_img = None
    current_pil_img = None
    predicted_class, confidence = None, None

    while True:
        '''
        (win11è£¡ä¸work)
        # è¦–çª—é—œé–‰åµæ¸¬ (é›™é‡æª¢æŸ¥æ³•)
        try:
            if cv2.getWindowProperty(win_name, cv2.WND_PROP_VISIBLE) < 1: break
        except: break
        '''
        key = cv2.waitKey(30) & 0xFF
        if key == ord('q'): break
        
        # è™•ç† Open æŒ‰éˆ•æˆ–æŒ‰éµ
        if params['clicked_open'] or key == ord('o'):
            root = tk.Tk(); root.withdraw()
            file_path = filedialog.askopenfilename()
            root.destroy()
            if file_path:
                current_pil_img = Image.open(file_path)
                predicted_class, confidence = run_logic(engine, current_pil_img, class_names)
            params['clicked_open'] = False

        # è™•ç†å‰ªè²¼ç°¿
        try:
            cb_img = ImageGrab.grabclipboard()
            if isinstance(cb_img, Image.Image):
                if last_clipboard_img is None or cb_img.size != last_clipboard_img.size:
                    current_pil_img = cb_img
                    last_clipboard_img = cb_img
                    predicted_class, confidence = run_logic(engine, current_pil_img, class_names)
        except: pass

        # é¡¯ç¤ºè™•ç†
        if current_pil_img:
            display_img = cv2.cvtColor(np.array(current_pil_img.convert('RGB')), cv2.COLOR_RGB2BGR)
            display_img = cv2.resize(display_img, (600, 500))
        else:
            display_img = np.zeros((500, 600, 3), dtype=np.uint8)

        # UI ç¹ªè£½
        bx, by, bw, bh = OPEN_BTN_RECT
        cv2.rectangle(display_img, (bx, by), (bx + bw, by + bh), (0, 255, 0), -1)
        cv2.putText(display_img, "OPEN FILE", (bx + 10, by + 28), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)
        
        if predicted_class:
            color = (0, 255, 0) if predicted_class != "Unknown" else (0, 0, 255)
            text = f"TFLite: {predicted_class} ({confidence:.1f}%)"
            cv2.putText(display_img, text, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)

        cv2.imshow(win_name, display_img)

    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()